{
  
    
        "post0": {
            "title": "Downloading daily stock data",
            "content": "import yfinance as yf import pymongo from tqdm import tqdm from datetime import datetime import pandas as pd import borsdata_api as api api = api.BorsdataAPI(open(&quot;api.txt&quot;, &quot;r&quot;).read()) # Börsdata is used as data source client = pymongo.MongoClient(&#39;192.168.1.38&#39;, 27017) # MongoDB server connection daily = client.prod.daily # The collection used to store daily data . Defining aggregation query and getting the symbols . It is important not to add duplicate data to the database. This can be solved by quering the database for all the dates that are already present in the database and then ignore these dates when adding the new data. This can be done with the following MongoDB aggregation query. It essencially matches the wanted stock symbol and then creates a list with all the dates. This cell also contains the query for all the stock symbols. These are stored in a separate collection called stock_info. The list of symbols returned will be used to tell the API what stocks to download. . aggreg_query = [ { &#39;$match&#39;: {&#39;symbol&#39;: None} }, { &#39;$group&#39;: { &#39;_id&#39;: &#39;$symbol&#39;, &#39;index&#39;: {&#39;$push&#39;: &#39;$date&#39;} } } ] symbols = [(stock[&quot;yahoo&quot;], stock[&quot;insId&quot;]) for stock in client.prod.stock_info.find()] print(&quot;Number of symbols:&quot;, len(symbols)) . Number of symbols: 1780 . Download loop . This is where the magic happens. This for-loop loops through the list of symbols previously retrieved and performes these steps: . Downloads the data. | Removes rows with NaN values. | Detaches the index (dates) and adds it as a column. | Makes all the column names lowercase. | Adds the symbol to every row. This is an important field for when every row becomes a document. | Adds the date and time when the data was retrieved. | Adds a string as the primary key. This a second security measure to prevents the adding of the same date twice to the database. | Removes the already present rows from the data we are going to add to the database. This uses the abovementioned aggregation query. | Finally, it converts the tabular data to documents (dictionaries) and inserts them into the database. | . This loop may take a long time to finish. Especially when the number of stocks is large. Some APIs might stop giving you data by blocking your requests a long time before all the symbols have been looped through. This is one of the reasons by Börsdata is used. Although it is a payed service, it does not limit the number of requests. If this is not an option for you, YahooFinance&#39;s API can be used. The pros are that they have all the stocks you could ever want and are free, but they start blocking requests after a couple of hundred queries. . progress = tqdm(symbols) for symbol, insId in progress: progress.set_description(symbol) # Set progress bar description df = api.get_instrument_stock_prices(insId) # Download the data #df = data[symbol] df = df.dropna() df.reset_index(level=0, inplace=True) df.columns = [column.lower() for column in df.columns] df[&quot;symbol&quot;] = symbol df[&quot;updated&quot;] = datetime.now() df[&quot;_id&quot;] = df[&quot;symbol&quot;] + &quot;:&quot; + df[&quot;date&quot;].astype(str) aggreg_query[0][&quot;$match&quot;][&quot;symbol&quot;] = symbol index = list(daily.aggregate(aggreg_query)) if len(index)&gt;0: # Check if stock already in db index=index[0] index = pd.Series(index[&quot;index&quot;]) df = df[~df[&#39;date&#39;].isin(index)] # Ignoring indicies already in db if len(df)&gt;0: daily.insert_many(df.to_dict(&#39;records&#39;)) . Cronjob . To make the data retrieval automatic it has to be scheduled. This can easily be done using cronjob on linux machines. The following script will be made into a cronjob and this parameter will be added to the crontab 0 18 * * 1-5 /home/ubuntu/data_downloader/daily_downloader.py. The parameter means that the command should be run every weekday at 18 o&#39;clock. . #- #!/home/ubuntu/data_downloader/env/bin/python import yfinance as yf import pymongo from tqdm import tqdm from datetime import datetime import pandas as pd import borsdata_api as api api = api.BorsdataAPI(open(&quot;api.txt&quot;, &quot;r&quot;).read()) # Börsdata is used as data source client = pymongo.MongoClient(&#39;192.168.1.38&#39;, 27017) # MongoDB server connection daily = client.prod.daily # The collection used to store daily data aggreg_query = [ { &#39;$match&#39;: {&#39;symbol&#39;: None} }, { &#39;$group&#39;: { &#39;_id&#39;: &#39;$symbol&#39;, &#39;index&#39;: {&#39;$push&#39;: &#39;$date&#39;} } } ] symbols = [(stock[&quot;yahoo&quot;], stock[&quot;insId&quot;]) for stock in client.prod.stock_info.find()] print(&quot;Number of symbols:&quot;, len(symbols)) progress = tqdm(symbols) for symbol, insId in progress: progress.set_description(symbol) # Set progress bar description df = api.get_instrument_stock_prices(insId) # Download the data #df = data[symbol] df = df.dropna() df.reset_index(level=0, inplace=True) df.columns = [column.lower() for column in df.columns] df[&quot;symbol&quot;] = symbol df[&quot;updated&quot;] = datetime.now() df[&quot;_id&quot;] = df[&quot;symbol&quot;] + &quot;:&quot; + df[&quot;date&quot;].astype(str) aggreg_query[0][&quot;$match&quot;][&quot;symbol&quot;] = symbol index = list(daily.aggregate(aggreg_query)) if len(index)&gt;0: # Check if stock already in db index=index[0] index = pd.Series(index[&quot;index&quot;]) df = df[~df[&#39;date&#39;].isin(index)] # Ignoring indicies already in db if len(df)&gt;0: daily.insert_many(df.to_dict(&#39;records&#39;)) .",
            "url": "https://nicoloridulfo.github.io/blog/2021/06/28/Data-Retrieval.html",
            "relUrl": "/2021/06/28/Data-Retrieval.html",
            "date": " • Jun 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nicoloridulfo.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nicoloridulfo.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}