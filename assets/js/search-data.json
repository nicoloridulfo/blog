{
  
    
        "post0": {
            "title": "Writing a LLVM compiler using Python",
            "content": "Recently I have been fascinated by the world of compiled programming languages. My current day to day work mainly involves interpreted languages such as Python and TypeScript. These languages are great for productivity, but they leave a lot to be desired when it comes to performance and resource usage (see benchmarks). In order to learn more about compiled languages I started writing small programs in C++, go and Rust. Rust is awesome! In 13 lines of code a rust noob can managed to write a function that looks for a string (in lowercase and uppercase) inside of a SHA256 hash. The kicker, it is fully parallel thanks to the rayon library! This is usually not that easy to do in other system languages. The only difference between the single threaded and parallel code is .into_par_iter() part. . pub fn find_hash(name: &amp;str) -&gt; String { let name_upper = &amp;name.to_uppercase(); let result = (0u64..u64::MAX).into_par_iter().find_any(|&amp;i| { let mut hasher = Sha256::new(); hasher.update(i.to_string().as_bytes()); let hash = base64::encode(&amp;hasher.finalize()); return hash.contains(name) || hash.contains(name_upper); }); let mut hasher = Sha256::new(); hasher.update(result.unwrap().to_string().as_bytes()); let hash = base64::encode(&amp;hasher.finalize()); hash } . There is something that Rust has in common with that this post is about, it compiles down to LLVM intermediate representation (IR). LLVM is basically a set of tools for developers to make compilers. A developer just needs to get their code into IR. After that, LLVM can optimize the IR for various metrics and then either run it in a jitted execution engine or compile it to machine code. One other nice thing about IR is that it is &quot;portable&quot; (we&#39;ll come to why I wrapped portable in quotes). There is no need to rewrite your compiler for every system architecture. . Brainfuck . In order to learn LLVM I decided to implement a compiler for one of the simplest programming languages: brainfuck(BF). It is a tape based language that only has these operators: + - &lt; &gt; [ ] . ,. When a BF program starts an array of 8 bit integers is initialized with zeros, this is the tape. There is also a pointer that we&#39;ll call tape_ptr that points to the &quot;current&quot; element in the array. A program might look something like this: +++.. This means increment the integer at the tape_ptr by one three times, then write the integer at the tape_ptr to the standard output (in our case the console). One would think that a three would be printed, but the console interprets the integer as a char type. A char of value “3” does not display. If we wanted a three to print we would have to write -[--&gt;+&lt;]&gt;.. This program decrements the current element (which overflows into 255). Then it enters a sort of while-loop given by []. The code between the brackets is looped &quot;while the current element is not zero&quot;. The &gt; and &lt; shift the position of the tape_ptr either to the right or left. The program therefore does the following: . - decrement the current position | [--&gt;+&lt;] -- decrement the current element 5 times | &gt; move one step to the right | + increment the current element | &lt; move on step to the left (the initial position) | ] is the value at the current element zero? if TRUE continue to the next operation (to the right) | if FALSE go back to the previous [ | . | . | . write the current value to stdout | . The result is that the element in the second position is incremented once for every time the element in the first position is decremented five times. This gives 255/5=51, where 255 is the overflown integer and 51 is the ascii character &#39;3&#39;. We will ignore the , instruction as it is quite useless; I have personally never used it. It is used for reading from stdin. However, if one is interested in writing programs in BF one should implement it in order to be able to use the language fully! . Now to the compiler . In unrelated project have used a python library called Numba a lot to speed up numerical computations. It manages to be so fast by compiling a subset of python down to IR which is then run on LLVM&#39;s jit execution engine. It can easily speed up python by 24 times and come close to C++ speeds. Numba uses a library called llvmlite to generate the IR. This is the library we are going to be using. . This whole project has really been a brainfuck. The documentation has been much thinner than I am used and, in some cases, nonexistent. To learn how to make system calls to the operating system kernel in order to write to stdout I had to read what registers to populate with what values by looking at source code. Because we are doing this on... Darwin ARM64! Which wants to have values in other registers than the linux on ARM64, or so it seems depending on what documentation you read. At this level, the error messages stop being helpful. When trying to implement the syscalls the kernel would just say things like: . zsh:invalid system call ./a.out Ok thanks, very useful! . Enough, let&#39;s start coding: . from llvmlite import ir code=&quot;&quot; counter = 0 def block_namer(): global counter counter += 1 return &quot;block_%d&quot; % (counter-1) TAPE_LEN = 10 mod = ir.Module(&quot;MainModule&quot;) mod.triple = &quot;arm64-apple-macosx12.0.0&quot; lfunc = ir.Function(mod, ir.FunctionType(ir.IntType(8), []), &quot;main&quot;) entry_block = lfunc.append_basic_block(&#39;entry&#39;) builder = ir.IRBuilder(entry_block) exit_block = builder.append_basic_block(&quot;exit&quot;) mod . ; ModuleID = &#34;MainModule&#34; target triple = &#34;arm64-apple-macosx12.0.0&#34; target datalayout = &#34;&#34; define i8 @&#34;main&#34;() { entry: exit: } . The compiler starts with these lines. It defines a main function where, like many languages, is where the program starts executing. More specifically, in the entry code block. We also create an exit block that we will later use to exit the program. Now let’s create our data structures. . tape = builder.alloca(ir.ArrayType(ir.IntType(8), TAPE_LEN)) builder.store(ir.Constant(ir.ArrayType(ir.IntType(8), TAPE_LEN), [0] * TAPE_LEN), tape) # Create tape pointer tape_ptr = builder.gep( tape, [ir.Constant(ir.IntType(8), 0), ir.Constant(ir.IntType(8), 0)]) mod . ; ModuleID = &#34;MainModule&#34; target triple = &#34;arm64-apple-macosx12.0.0&#34; target datalayout = &#34;&#34; define i8 @&#34;main&#34;() { entry: %&#34;.2&#34; = alloca [10 x i8] store [10 x i8] [i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0], [10 x i8]* %&#34;.2&#34; %&#34;.4&#34; = getelementptr [10 x i8], [10 x i8]* %&#34;.2&#34;, i8 0, i8 0 exit: } . We start by allocating an array of 8-bit integers of the length TAPE_LEN on the stack. This comes out to %&quot;.2&quot; = alloca [10 x i8]. The variable that is returned %.2 is a pointer to the array. Notice @ and %, these indicate that the variable is either global or local. For example, the function @&quot;main&quot; is global, but the pointer to the array %&quot;.2&quot; is local. . The next instruction initializes the array with zeroes. Notice the [10 x i8]* %&quot;.2&quot;, it refers to the pointer we created earlier as it is there we are storing the zeroes. LLVM IR is strictly typed, and the types are mentioned every time a variable is used. It says that %.2 is a pointer to an array of ten eight-bit integers. . In the following instruction we create a pointer to the first element of the array and store it in %.4 . In order to handle the while loops in the language we use LLVM IR&#39;s blocks. They work like labels that you can conditionally or unconditionally branch to. We keep track of the branches using a python stack (just a regular list). One issue I was having when I first started writing this compiler is that I was doing too much computation in python. I was basically precomputing the whole BF program. This is usually allowed (and recommended) for performance reasons. But then, why even write a compiler? So, the branching had to be done in BF. . blocks = [builder.append_basic_block(block_namer())] builder.branch(blocks[0]) builder = ir.IRBuilder(blocks[0]) mod . ; ModuleID = &#34;MainModule&#34; target triple = &#34;arm64-apple-macosx12.0.0&#34; target datalayout = &#34;&#34; define i8 @&#34;main&#34;() { entry: %&#34;.2&#34; = alloca [10 x i8] store [10 x i8] [i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0], [10 x i8]* %&#34;.2&#34; %&#34;.4&#34; = getelementptr [10 x i8], [10 x i8]* %&#34;.2&#34;, i8 0, i8 0 br label %&#34;block_0&#34; exit: block_0: } . The following code takes a BF instructions and translates them into the corresponding IR instructions. . for op in code: if op == &quot;+&quot;: val_at_ptr = builder.load(tape_ptr) builder.store(builder.add( val_at_ptr, ir.Constant(ir.IntType(8), 1)), tape_ptr) elif op == &quot;-&quot;: val_at_ptr = builder.load(tape_ptr) builder.store(builder.sub( val_at_ptr, ir.Constant(ir.IntType(8), 1)), tape_ptr) elif op == &quot;&gt;&quot;: tape_ptr = builder.gep(tape_ptr, [ir.Constant(ir.IntType(8), 1)]) elif op == &quot;&lt;&quot;: tape_ptr = builder.gep(tape_ptr, [ir.Constant(ir.IntType(8), -1)]) elif op == &quot;.&quot;: # https://go.dev/src/syscall/zsysnum_darwin_arm64.go fty = ir.FunctionType(ir.IntType(32), [ ir.IntType(32), # x16 (=4) ir.IntType(32), # x0 (=1) ir.IntType(8).as_pointer(), ir.IntType(32) ]) # Uncomment this to make char=3 =&gt; &quot;3&quot; # char = builder.add(char, ir.Constant(ir.IntType(8), 48)) builder.asm(fty, &quot;svc 0&quot;, &quot;=r,{x16},{x0},{x1},{x2}&quot;, ( ir.IntType(32)(4), ir.IntType(32)(1), tape_ptr, ir.IntType(32)(1) ), True, name=&quot;print&quot;) elif op == &quot;e&quot;: builder.asm(ir.FunctionType(ir.IntType(32), [ir.IntType(32), ir.IntType(32)]), &quot;svc 0&quot;, &quot;=r,{x0},{x16}&quot;, [ir.IntType(32)(8), ir.IntType(32)(1)], True, name=&quot;asm_add&quot;) elif op == &quot;[&quot;: # Create a new block # Make the current block branch to the new block blocks.append(builder.append_basic_block(block_namer()+&quot;_open&quot;)) builder.branch(blocks[-1]) builder = ir.IRBuilder(blocks[-1]) elif op == &quot;]&quot;: # Create a new block # Make the current block branch to the new block if the value at the current pointer is 0 val_at_ptr = builder.load(tape_ptr) branch_condition = builder.icmp_signed( &#39;==&#39;, val_at_ptr, ir.Constant(ir.IntType(8), 0)) open_block = blocks.pop() close_block = builder.append_basic_block( open_block.name.replace(&quot;open&quot;, &quot;close&quot;)) builder.cbranch(branch_condition, close_block, open_block) builder = ir.IRBuilder(close_block) . Increment &amp; Decrement . There is a lot of code to unpack here. Let’s start with the + and - instructions. . val_at_ptr = builder.load(tape_ptr) builder.store(builder.add( val_at_ptr, ir.Constant(ir.IntType(8), 1)), tape_ptr) . The IR will look something like this: . %&quot;.6&quot; = load i8, i8* %&quot;.4&quot; %&quot;.7&quot; = add i8 %&quot;.6&quot;, 1 store i8 %&quot;.7&quot;, i8* %&quot;.4&quot; . Remember that the pointer to the current element was .4 It loads the value of the current element into .6, then add 1 to it and finally stores it where .4 points. . Move tape pointer . tape_ptr = builder.gep(tape_ptr, [ir.Constant(ir.IntType(8), 1)]) . To move the tape pointer, we simply use the get element pointer function. This is similar to incrementing a pointer in regular C, but the function handles all the logistics with how many bytes to jump. . Loops . For [: . blocks.append(builder.append_basic_block(block_namer()+&quot;_open&quot;)) builder.branch(blocks[-1]) builder = ir.IRBuilder(blocks[-1]) . First, we create a new block and add it to the list of blocks. We make the current block branch to the new block unconditionally. This is because LLVM IR does not simply move to the next block after it has finished the instructions is the current block. It must explicitly be told to move to the block below (meh). . For ]: . val_at_ptr = builder.load(tape_ptr) branch_condition = builder.icmp_signed(&#39;==&#39;, val_at_ptr, ir.Constant(ir.IntType(8), 0)) open_block = blocks.pop() close_block = builder.append_basic_block(open_block.name.replace(&quot;open&quot;, &quot;close&quot;)) builder.cbranch(branch_condition, close_block, open_block) builder = ir.IRBuilder(close_block) . The logic is quite similar. But in this case whether we move to the next block depends on whether the value at the current pointer is zero when we are at the end of the loop. . For a program like this +[&gt;+[&gt;+&lt;-]&lt;-]&gt;&gt; the branching will look like this. . define i8 @&quot;main&quot;() { entry: ... br label %&quot;block_0&quot; exit: ... ret i8 %&quot;.35&quot; block_0: ... br label %&quot;block_1_open&quot; block_1_open: ... br label %&quot;block_2_open&quot; block_2_open: ... br i1 %&quot;.24&quot;, label %&quot;block_2_close&quot;, label %&quot;block_2_open&quot; &lt;-- This is the end of the inner loop block_2_close: ... br i1 %&quot;.31&quot;, label %&quot;block_1_close&quot;, label %&quot;block_1_open&quot; &lt;-- This is the end of the outer loop block_1_close: .. br label %&quot;exit&quot; . Now to the hard part... &#128128; . We want to be able to print the current element&#39;s value to stdout. In order to do that the program has to talk to the kernel though a syscall. Think of it as calling on the operating system’s API that does things like write or read to stdout or files, open sockets, mount and unmount drives, change the permissions of files and much much more. The problem is that LLVM has nothing to do with the kernel. This is usually handled by local libraries. We therefore must implement this by writing native assembly. This is the part where the program becomes system architecture specific. I think that this is where we would normally be linking to local libraries that would handle this for us. . Since I am writing this on a ARM64, that is the syscall table I have to look at. As I am writing this, I wanted to add a link to the syscall table for Darwin ARM64, but I cannot find any. During the development process I scoured various forums, blogs and source code files on github to piece together how to make the correct syscall. This might be the best table I have found. However, it does not fully work on my machine. Depending on who you ask, Darwin ARM64 either uses register X16 or X8 to specify what syscall to make and uses either the value 0x40 or 4 or 0x2000004 to specify that we want to write. I have no idea, I must have tried all combinations of registers and values. . Making the syscall . fty = ir.FunctionType(ir.IntType(32), [ ir.IntType(32), # x16 (=4) ir.IntType(32), # x0 (=1) ir.IntType(8).as_pointer(), # x1 ir.IntType(32) # x2 ]) builder.asm(fty, &quot;svc 0&quot;, &quot;=r,{x16},{x0},{x1},{x2}&quot;, ( ir.IntType(32)(4), ir.IntType(32)(1), tape_ptr, ir.IntType(32)(1) ), True, name=&quot;print&quot;) . First, we create a function type, it is basically a call signature. The assembly will return a 32-bit integer and take the following inputs. The &quot;inputs&quot; are values that we are going to put in the registers {x16},{x0},{x1},{x2}. . 32-bit integer - The type of syscall (write call) | 32-bit integer - Where to write (stdout) | 8-bit integer - Pointer to the char array (in our case only the current element) | 32-bit integer - The length of the char array | Luckily, the llvmlite library has a convenient way to write out values to the registers automatically without us having to move every value into the registers. We just specify the &quot;constraint&quot; &quot;=r,{x16},{x0},{x1},{x2}&quot; and then pass the values in the args parameter . ir.IntType(32)(4), ir.IntType(32)(1), tape_ptr, ir.IntType(32)(1) . The actual assembly is the svc 0 instruction. . I also implemented an exit instruction to BF that can be used by writing e. It makes an exit syscall. . builder.asm(ir.FunctionType(ir.IntType(32), [ir.IntType(32), ir.IntType(32)]), &quot;svc 0&quot;, &quot;=r,{x0},{x16}&quot;, [ir.IntType(32)(8), ir.IntType(32)(1)], True, name=&quot;exit&quot;) . And that was pretty much it! The full code can be found in on my github. I hope you enjoyed this post! .",
            "url": "https://nicolo.io/computer%20science/lowlevel/compilers/2022/02/06/Brainfuck-LLVM-compiler.html",
            "relUrl": "/computer%20science/lowlevel/compilers/2022/02/06/Brainfuck-LLVM-compiler.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Building a Blockchain from scratch",
            "content": "What will be covered . My goals | What is a blockchain | Building a blockchain from scratch | My implementation | Future development | Appendix A | . Goals . My goal with this post and the code I have and will be writing is to get a solid understanding of how blockchains and crypto currencies work from the ground up. There is no better way to learn something that by doing it yourself. Firstly, I will implement a blockchain (this post), thereafter I will use the blockchain to build a distributed ledger and finally I shall build a crypto currency. . What is a blockchain . A blockchain is essentially just a linked list with extra features. These features make it hard to tamper with the data and easy to check the validity of the data. People use blockchains for many things. Most popularly for crypto currencies. Since it is anonymous individuals that manage the creation and validation of transactions. One wouldn&#39;t want someone to go back and change their account balance in order to make them rich. The currency is safe for as long as most individuals are honest. . Implementing a Blockchain . We will now implement a blockchain from scratch. To illustrate this, we will first implement a simple linked list and then build upon it until it becomes a blockchain. . Linked List . As the name suggests a linked list is a list where the data is linked together. An array on the other hand is data in memory stored in adjacent memory addresses. Note that the usual term for the object that stores the data in a linked list is node, however in blockchains the term is block. . Normally one would implement a singly linked list like this: . class Node: next_node = None def __init__(self, data:str=None): self.data = data class LinkedList: head=None def add(self, data:str): if self.head: current_node = self.head while current_node.next_node: current_node = current_node.next_node current_node.next_node = Node(data) else: # The first node gets an empty string as prevous hash because there are not previous nodes self.head = Node(data) . The problem with a simple linked list is that you can change the data anywhere in the list. Like so: . linked_list = LinkedList() for n in range(10): linked_list.add(str(n)) # Change the value of the third node linked_list.head.next_node.next_node.data = 12897 . Adding the hash of the previous block . We do not want anyone to change the data, so we add the hash of the previous node to the next node&#39;s fields. Now to check if the data in a node has been tampered with, we just recompute the hash of the previous node and compare with the current nodes &quot;hash of the previous block&quot;. . A quick refresher about hashes. A hash is the output of a hash function. Sometimes referred to as a fingerprint of some piece of data. A hash function is a so called &quot;one way&quot; function. You can easily compute the hash of a piece of data, but it is very hard to go from the hash to the original piece of data. The only way is to use &quot;brute-force&quot; and try all different combinations of inputs. If the original piece of data is longer than the hash, it is impossible since information has been lost. As shown in the example below, just a small change in the input creates a completely different output. Despite this, the function is non probabilistic, which means that a input will always have the same output. . from hashlib import sha256 def hasher(input: str): return sha256(input.encode()).hexdigest() print(hasher(&quot;Hello World&quot;)) print(hasher(&quot;Hello World!&quot;)) . a591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e 7f83b1657ff1fc53b92dc18148a1d65dfc2d4b1fa3d677284addd200126d9069 . To add the hash of the previous block we add the field previous_node_hash to the node. . class Node: next_node = None def __init__(self, previous_node_hash:str, data:str=None): self.data = data self.previous_node_hash = previous_node_hash class LinkedList: head=None def add(self, data:str): if self.head: current_node = self.head while current_node.next_node: current_node = current_node.next_node current_node_hash = sha256(str(current_node.__dir__).encode()).hexdigest() # &lt;-- current_node.next_node = Node(current_node_hash, data) else: self.head = Node(&quot;&quot;, data) . A node now looks like this when printed . linked_list = LinkedList() for n in range(10): linked_list.add(str(n)) print(linked_list.head.next_node.__dict__) . {&#39;data&#39;: &#39;1&#39;, &#39;previous_node_hash&#39;: &#39;0e69ce586d540294b97a90e1f224ffd1af1d672007cea72221fc18955550051e&#39;, &#39;next_node&#39;: &lt;__main__.Node object at 0x108c045e0&gt;} . We now have officially created the most basic blockchain! Now, you can still change the value of Node.data. But that would make the next node&#39;s Node.prevous_node_hash incorrect. So, you would have to recompute the hash of the edited node and update the next node. However, now the hash in the following node does not match the one after that. You would have to change all the hashes until you arrive at the last node. In the case of crypto currencies, imagine that it takes monstrous amount of computation to add just a single node to the list and you start to realize why it is infeasible to edit the record. While you are changing the record and recomputing the whole chain, hundreds of thousands of computers are trying to add new nodes to the end of chain. You would never catch up. . Bitcoin and PyChain split the information in the node (from now on referred to as block) into two sections: a header and a body. This is done in order to save space and computation. For a lot of operations, you do not need the body; you just need a hash of it in order to validate the data should you want to. . Since PyChain is a general purpose blockchain, it can also discard some header fields that are present in bitcoin&#39;s header. For example: version, difficulty target and nonce. All of these are application specific and are important for bitcoin, but not to PyChain. Perhaps a field could be added to PyChain&#39;s header that could be used to store some information. . Implementing PyChain . PyChain is extremely simple, let&#39;s implement part of it: . Block . A block is defined as follows: . ======HEADER========== 4 bytes (I): block number : 0:4 32 bytes (32s): previous block hash: 4:36 32 bytes (32s): block body hash : 36:68 8 bytes (Q): block creation time: 68:76 ======END HEADER====== ======BODY============ n bytes (ns): block body : 76:n ======END BODY======== . The index of the block in the chain, the previous block&#39;s hash, the hash of the body, the time when the block was created and the body. The header is of fixed size, always 76 bytes. However, the body has a variable size, but is easy to compute: len(block)-76. . PyChain uses struct in order to encode and decode blocks. . import struct # a library to create C-style structs def encode_block(n: int, prev_hash: bytes, time: int, body: str): &quot;&quot;&quot; Encode a block Packs the block into an array of bytes :param n: block number :param prev_hash: previous block hash :param time: time of block creation :param body: block body &quot;&quot;&quot; block = struct.pack(f&quot;=I32s32sQ{len(body)}s&quot;, n, prev_hash, sha256(body.encode()).digest(), time, body.encode()) return block def decode_block(block: bytes): &quot;&quot;&quot; Decode a block Unpacks the block into an array of bytes Note: the 76 in the format string is the length of the header :param block: block to decode &quot;&quot;&quot; n, prev_hash, body_hash, time, body = struct.unpack( f&quot;=I32s32sQ{len(block)-76}s&quot;, block) return n, prev_hash, body_hash, time, body.decode() . These functions basically just take several values and either packs or unpacks them in order to achieve a C-like space efficiency. Easy as that. If you are unfamiliar with this library this is how it works: . import struct print(struct.pack(&quot;2I&quot;, 123, 456)) . b&#39;{ x00 x00 x00 xc8 x01 x00 x00&#39; . print(struct.unpack(&quot;2I&quot;, b&#39;{ x00 x00 x00 xc8 x01 x00 x00&#39;)) . (123, 456) . The format of a block is I32s32sQ{len(body)}s. Where Is are integers, ss are char arrays and Qs are unsigned longs. The number before the letter specifies how many of that type there are. Since the body has a variable number of s, it can be calculated using {len(block)-76}s. . To add a block to the chain we simply: . class BlockChain: blocks=[] HEADER_SLICE = slice(0, 76) def add_block(self, body: str): if len(blocks)==0: raise Exception(&quot;No genesis block&quot;) prev_header = self.blocks[-1][self.HEADER_SLICE] # Get the header of the previous block prev_hash = sha256(prev_header).digest() block = self.encode_block( len(self.blocks), prev_hash, int(time()), body) self.blocks.append(block) . Performance of PyChain . Speed . Since adding a block to the chain simply requires you to compute one hash and pack the data it is extremely fast. It takes only 24ms to add 10K blocks to the chain. That is 2.352 microseconds per block. The reason distributed blockchains take so long (approx. 10min for bitcoin) is because consensus is needed. In bitcoins case, to add one block hundreds of thousands of computers need to find a value that results in the header&#39;s hash starting with n number of zeros. The number of calculations needed to do this are astronomical. . Space . A blockchain containing 10K blocks (with empty bodies) would be 10000 blocks x 76 bytes = 760 KB. This is slightly better than bitcoin which has a header of 80 bytes. . Future development . PyChain needs some unit tests and more chain validation features in order to move to the next step. . The following step is to create a peer-to-peer protocol that would enable this blockchain implementation to be distributed. Of the top of my head, that would require the following abilities: . broadcast new block | get blocks (ask for blocks from peers) | . Appendix A . Fun fact! The current version of PyChain can be minified and fit in 31 unreadable lines of code: . O, N, M, L, H, B, C = round, staticmethod, isinstance, range, print, slice, len from hashlib import sha256 as E from time import time as G import struct as D from typing import List class K: blocks=[];HEADER_SLICE=B(4,80);BLOCK_NR_SLICE=B(4,8);PREV_HASH_SLICE=B(8,40);BODY_HASH_SLICE=B(40,72);BLOCK_CREATION_TIME_SLICE=B(72,80);BODY_SLICE=B(80,None) def add_block(A,body):B=A.blocks[-1][A.HEADER_SLICE];D=E(B).digest();F=A.encode_block(C(A.blocks),D,int(G()),body);A.blocks.append(F) def verify_chain(A): P=&#39;=Q&#39;;O=True;F=False if C(A.blocks)==0:return O for B in L(1,C(A.blocks)): G=E(A.blocks[B-1][A.HEADER_SLICE]).digest();H=A.blocks[B][A.PREV_HASH_SLICE] if G!=H:return F I=A.blocks[B][A.BODY_HASH_SLICE];J=E(A.blocks[B][A.BODY_SLICE]).digest() if I!=J:return F K=D.unpack(&#39;=I&#39;,A.blocks[B][A.BLOCK_NR_SLICE])[0] if K!=B:return F M=D.unpack(P,A.blocks[B-1][A.BLOCK_CREATION_TIME_SLICE])[0];N=D.unpack(P,A.blocks[B][A.BLOCK_CREATION_TIME_SLICE])[0] if N&lt;M:return F return O def import_chain(B,chain): A=chain if M(A,list):B.blocks=A elif M(A,bytes):B.blocks=[A] else:raise Exception(&#39;Invalid import data&#39;) def export_chain(A):return A.blocks @N def encode_block(n,prev_hash,time,body):A=body;B=D.pack(f&quot;=2I32s32sQ{C(A)}s&quot;,C(A),n,prev_hash,E(A.encode()).digest(),time,A.encode());return B @N def decode_block(block):A=block;B,E,F,G,H,I=D.unpack(f&quot;=2I32s32sQ{C(A)-80}s&quot;,A);return B,E,F,G,H,I.decode() .",
            "url": "https://nicolo.io/computer%20science/blockchain/2022/01/16/blockchains.html",
            "relUrl": "/computer%20science/blockchain/2022/01/16/blockchains.html",
            "date": " • Jan 16, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Finding Support and Resitance levels",
            "content": "Support and resitance levels (SRL) are important price levels at which the price of a stock might bounce or in some cases penetrate leading to a change in sentiment that may fuel a new trend. SRLs can be horizontal, but may also be diagonal. The latter are also known as trendlines. These will be investigated in the next post. Finding horizontal SRLs is usually done manually by finding price reversal points and then connecting these points using a line. Humans are quite good at drawing lines connecting points as we can see what points are important and which ones are not. Machines have a more difficult time understanding this. . After identification, SRLs may be used in trading in a number of different ways. My favorites are &quot;break out&quot; and &quot;bounce&quot; strategies. The first one buys a stock when the price penetrates an important level. While the second one bets on the price level not being penetrated as the stock bounces on the price level. . In this post, I will evaluate three different techniques that can be used for finding SRLs. These techniques are: . Volume Profile | Counting | K-Means | . But first, as always, some importing of libraries. . import pymongo from tqdm import tqdm import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.pylab as pylab; pylab.rcParams[&#39;figure.figsize&#39;] = 20, 15 import mplfinance as mpf from scipy.signal import argrelextrema import pandas_ta as ta from collections import Counter from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score . Import data . Now let&#39;s import the data from the database and plot the last stock. . client = pymongo.MongoClient(&#39;192.168.1.38&#39;, 27017) daily = client.prod.daily symbols = [stock[&quot;yahoo&quot;] for stock in client.prod.stock_info.find()] all_dfs = dict() for symbol in tqdm(symbols[:10]): df = pd.DataFrame(daily.find({&quot;symbol&quot;:symbols[1]}).sort(&quot;date&quot;, pymongo.ASCENDING)) df.set_index(&quot;date&quot;, inplace=True) all_dfs[symbol] = df mpf.plot(df[:200], type=&#39;candle&#39;) . 100%|██████████| 10/10 [00:14&lt;00:00, 1.49s/it] . Argrelextrema . First, in order to find SRLs reversal points need to be found. There are many ways of doing this. Popular methods include the zig-zag indiator or argrelextrema. The latter will be used in this post. This algorithms deems a point to be a reversal if it is the greatest (or smallest) point withing a window. Essentially, the algorithm compares n points before and after the current point. . The following is what we want, these are reversal points: . df[:200].close.iloc[argrelextrema(df[:200].close.values, np.greater_equal, order=4)[0]].head() . date 2001-07-03 129.2140 2001-07-13 116.4473 2001-07-19 116.4473 2001-08-02 93.2352 2001-08-14 94.3958 Name: close, dtype: float64 . Notice that argrelextrama has a parameter called &quot;order&quot;. This integer indicates how many points around the current point that should be checked. If this number is large, then only &quot;large reversals&quot; will be found. The opposite is true for small values of &quot;order&quot;. Larger values result in fewer reversals. . Now let&#39;s plot these reversals together with the stock price. The reversals will be found with order sizes of 4 and 10 respecively. . view = df[:200] fig, axs = plt.subplots(1, 2, figsize=(10, 5)) for ax, order in zip(axs, [4, 10]): peaks = view.close.iloc[argrelextrema(view.close.values, np.greater_equal, order=order)[0]] bottoms = view.close.iloc[argrelextrema(view.close.values, np.less_equal, order=order)[0]] ax.plot(view.close) ax.scatter(peaks.index, peaks, color=&quot;g&quot;) ax.scatter(bottoms.index, bottoms, color=&quot;r&quot;) for tick in ax.get_xticklabels(): tick.set_rotation(90) plt.show() . As it can be seen, the algorithm finds the peaks and bottoms of the stock price. Notice that around January 2002 the algorithm found multiple tops after each other. If the zig-zag indicator would have been used, then this output would not have been seen. The indicator requires there to be a bottom before a new top. . Note: notice that the point around October 2001 make up a head and should formation. In the coming posts these points will be used to find price action formations such as VCP. . Getting all the pivots . Just getting reversals for the close price can be useful, but the price might not close at the SRL. Sometimes the touch of the SRL is at the high or low of the day. Therefore, reversals for both the high, low and close have to be found. This is done using the get_pivots() function. A list of all prices where the stock reverses is returned. As the price of a stock is quite noisy, the prices are rounded to the closest integer. . The aforementioned function works well, but can be improved by incorporating different values for order. I chose to call this function multi_scale_pivots(). One feature of this function is that it weights larger values for order more. This is good as larger reversals are more important than smaller ones. The function does this by featuring the prices of bigger reversals more often (get_pivots(df, order=n)*(n-min(times))). . def get_pivots(df, order=4): pivots = list() pivots += list(df.low.iloc[argrelextrema(df.low.values, np.less_equal, order=order)[0]].values) pivots += list(df.high.iloc[argrelextrema(df.high.values, np.greater_equal, order=order)[0]].values) pivots += list(df.close.iloc[argrelextrema(df.close.values, np.less_equal, order=order)[0]].values) pivots += list(df.close.iloc[argrelextrema(df.close.values, np.greater_equal, order=order)[0]].values) pivots = [round(pivot) for pivot in pivots] return pivots def multi_scale_pivots(df, times=range(4, 10)): pivots = list() for n in times: pivots+=get_pivots(df, order=n)*(n-min(times)) return pivots print(&quot;get_pivots() - length:&quot;, len(get_pivots(df[:20]))) print(get_pivots(df[:20])) print(&quot; nmulti_scale_pivots() - length:&quot;, len(multi_scale_pivots(df[:20]))) print(multi_scale_pivots(df[:20])) . get_pivots() - length: 12 [115, 107, 92, 130, 130, 118, 118, 111, 93, 129, 116, 116] multi_scale_pivots() - length: 118 [115, 107, 92, 130, 130, 118, 118, 111, 93, 129, 116, 116, 115, 107, 92, 130, 130, 118, 111, 93, 129, 116, 115, 107, 92, 130, 130, 118, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 107, 92, 130, 130, 111, 93, 129, 116, 92, 130, 130, 93, 129, 116, 92, 130, 130, 93, 129, 116, 92, 130, 130, 93, 129, 116, 92, 130, 130, 93, 129, 116, 92, 130, 130, 93, 129, 116] . The following function has been created to plot the result of the models: . def plot_SR(df, func, size = 150): fig, axs = plt.subplots(2,2, figsize=(10, 10)) axs = axs.reshape(-1) for i, ax in enumerate(axs): view = df[size*i:size*(1+i)] sr = func(view) mpf.plot(view, ax = ax, type=&quot;candle&quot;, hlines={&quot;hlines&quot;:list(sr), &quot;alpha&quot;:0.3, &quot;colors&quot;:[&quot;r&quot;, &quot;g&quot;, &quot;b&quot;, &quot;c&quot;, &quot;m&quot;, &quot;y&quot;]}) for tick in ax.get_xticklabels(): tick.set_rotation(90) plt.show() . Volume Profile . The first model that is going to be used is based on the volume profile and has therefore been named such. The &quot;volume&quot; is the number of shares traded on a specific time interval (e.g. per day). See the volume column in the cell below. The volume profile measures the number of shares traded in a price interval. Large volumes at a specific price can indicate SRLs. One explanation might be that during a downward trend seller sell at lower and lower prices. At some point they are not willing to sell for less per share and buyers are stepping in and absorbing the sell pressure. It might take a lot of buying before the sentiment changes and the price changes direction. Therefore, quite some volume might be traded at the reversal price. At a future time, sellers might become buyers are prices which have previously acted as reversal zones. Thusly, even more volume is traded at that price. . The way that this technique is going to be applied is by calculating the volume profile and then choosing the n price levels with the highest volume. . df[[&quot;open&quot;, &quot;high&quot;, &quot;low&quot;, &quot;close&quot;, &quot;volume&quot;]].head(1) . open high low close volume . date . 2001-06-27 117.9948 | 119.5423 | 115.2867 | 117.6079 | 1088873 | . def volume_profile(view, bins = 10, n = 4): vp = view.copy().ta.vp(bins) sr = vp.iloc[np.argsort(vp[&quot;total_volume&quot;])[:n]][&quot;mean_close&quot;] return list(sr) plot_SR(df, volume_profile) . As it can be seen in the plots above, the levels chosen by the algorithm are not always exactly at the reversals. This can be explained by the fact that a lot of volume is also traded in consolidation. Sometimes more than at the reversals. These SRLs are more suitable as reversal zones rather than reversal prices. . K-Means . The second technique that is going to be evaluated will be named k-means and it uses machine learning to find the center of a cluster of points (for illustration see video by Najam Syed). In this case, the points are going to be prices and the centers will be the support and resitance points. This approach was inspired by Karl Tengelin in his master thesis in 2020. The difference between the following approach and Tengelin&#39;s approach is that he used tick data and in this case daily data will be used. . Following Tengelin&#39;s approach, all closing prices are given to the model. The model then tries to find the centers of what hopefully are clusters. This ML model&#39;s primary parameter is k. The value chosen for this parameter should be chosen such that the silloute score is maximised. The code below was heavily inspired by Suhail Saqan&#39;s blog post. . def k_means_all(view): def optimum_Kvalue(data): kmax = 11 sil = {} k_model = {} for k in range(2, kmax+1): kmeans = KMeans(n_clusters = k).fit(data) k_model[k] = kmeans labels = kmeans.labels_ sil[k]=(silhouette_score(data, labels)) optimum_cluster = k_model[max(sil, key=sil.get)] return optimum_cluster model = optimum_Kvalue(np.array(view.close).reshape(-1, 1)) return list(model.cluster_centers_.reshape(-1)) plot_SR(df, k_means_all) . The plots above do not look very good. Some have a lot of lines and some have very few. Very few are at SRLs. . One thing that can be optimized is to feed the model the reversals prices and make it cluster these. . def k_means_reversals(view): def optimum_Kvalue(data): kmax = 11 sil = {} k_model = {} for k in range(2, kmax+1): kmeans = KMeans(n_clusters = k).fit(data) k_model[k] = kmeans labels = kmeans.labels_ sil[k]=(silhouette_score(data, labels)) optimum_cluster = k_model[max(sil, key=sil.get)] return optimum_cluster model = optimum_Kvalue(np.array(multi_scale_pivots(view)).reshape(-1, 1)) return list(model.cluster_centers_.reshape(-1)) plot_SR(df, k_means_reversals) . These results are far better. The model is able to find good SRLs. One drawdown of this model is that the user is not able to tell it how many levels to find. By lowering the value of k the performance of the model may be impacted. . Count . The final method has been named count, because that is what it does. The number of time the price reverses at a price is counted. The higher count the better. This means that the user can chose to plot the n best lines by ranking the lines by count. . The following code is used to count: . def count(view, n = 7): pivots = multi_scale_pivots(view) count = Counter(pivots) count = {k: v for k, v in sorted(count.items(), key=lambda item: -item[1])} return list(count.keys())[:n] plot_SR(df, count) . This method performes on par with k-means, but has the advantage of ranking the results. . Comparison . Now the performance of the three methods will be compared. The model&#39;s outputs will be plotted together in order to see the difference. However, the figures can seem a little messy as there may in some places be a few lines in very close proximity. . These are the colors: . Volume Profile: RED | K-means: GREEN | Count: BLUE | . fig, axs = plt.subplots(2,2, figsize=(15, 15)) axs = axs.reshape(-1) for i, ax in enumerate(axs): view = df[150*i:150*(1+i)] vp = list(volume_profile(view)) km = list(k_means_reversals(view)) cn = list(count(view)) mpf.plot(view, ax = ax, type=&quot;candle&quot;, hlines={&quot;hlines&quot;:vp+km+cn, &quot;alpha&quot;:0.5, &quot;colors&quot;:[&quot;r&quot;]*len(vp) + [&quot;g&quot;]*len(km) + [&quot;b&quot;]*len(cn)}) plt.show() . In my opinion, the k-means&#39; SRLs are the best. The problem with the number of lines found can be solved by combining the k-means models with the count model. This is done by finding labels of all the samples and then sorting the labels from most to least frequently used. . This is the final result: . from collections import Counter, defaultdict def k_means_reversals_count(view, n=6): def optimum_Kvalue(data): kmax = 11 sil = {} k_model = {} for k in range(2, kmax+1): kmeans = KMeans(n_clusters = k).fit(data) k_model[k] = kmeans labels = kmeans.labels_ sil[k]=(silhouette_score(data, labels)) optimum_cluster = k_model[max(sil, key=sil.get)] return optimum_cluster model = optimum_Kvalue(np.array(multi_scale_pivots(view)).reshape(-1, 1)) clusters = model.cluster_centers_.reshape(-1) d = dict() for label, count in Counter(model.labels_).items(): d[clusters[label]] = count d = {k: v for k, v in sorted(d.items(), key=lambda item: -item[1])} return list(d.keys())[:n] plot_SR(df, k_means_reversals_count) . In the next post trendline finders will be created! .",
            "url": "https://nicolo.io/finance/analysis/2021/07/09/Support-Resistance.html",
            "relUrl": "/finance/analysis/2021/07/09/Support-Resistance.html",
            "date": " • Jul 9, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Downloading daily stock data",
            "content": "My ambition is to build a fully automated trading system that: . Downloads the latest data | Performs some cleaning steps | Generates trading signals | Opens and closes positions in accordance with a risk management framework | . This system will be built step by step in a series of posts. As the list suggests, the first step is to update the database with the latest data when it is available. First, what database are we using? . Database . There are many options when it comes to chosing a database. The most popular type of database for tabular data is a SQL database. This is due to their good performance and data structure. SQL databases are structured in a tabular fasion which is the same structure that pandas dataframes are structured. Noteworthy SQL databases are MariaDB, PostgreSQL and SQLite. However, a SQL database will not be used in this project for a number of reasons. Instead, a NOSQL database will be used. More specifically, MongoDB. But, why? First let&#39;s address the pros and cons of SQL databases. It is true that they usually have good performance. This can be a deciding factor, but for small amounts of data (&lt;50GB) there might not be any difference. NOSQL databases can be just as performant if queried correctly and if indicies are used. When it comes to the data structure, some restructuring of the data has to be performed before it can be inserted into a NOSQL database. But this is not a problem, many python libraries have functions such as dataframe.to_dict(&quot;records&quot;) that easily converts tabular data into a list of dictionaries suited for NOSQL databases. These arguments cancel eachother out, and either database type could be used. The final con of SQL databases that tips the needle in favor of NOSQL databases is that the python libraries that are used to access SQL databases have terrible APIs. Not only that, setting up such a database is also a painful process that takes a lot of time if it is to be done right. . Imports and connections . Now for the program used to download the data. The following imports and connections are going to be needed. . import yfinance as yf import pymongo from tqdm import tqdm from datetime import datetime import pandas as pd import borsdata_api as api api = api.BorsdataAPI(open(&quot;api.txt&quot;, &quot;r&quot;).read()) # Börsdata is used as data source client = pymongo.MongoClient(&#39;192.168.1.38&#39;, 27017) # MongoDB server connection daily = client.prod.daily # The collection used to store daily data . Defining aggregation query and getting the symbols . It is important not to add duplicate data to the database. This can be solved by quering the database for all the dates that are already present in the database and then ignore these dates when adding the new data. This can be done with the following MongoDB aggregation query. It essencially matches the wanted stock symbol and then creates a list with all the dates. This cell also contains the query for all the stock symbols. These are stored in a separate collection called stock_info. The list of symbols returned will be used to tell the API what stocks to download. . aggreg_query = [ { &#39;$match&#39;: {&#39;symbol&#39;: None} }, { &#39;$group&#39;: { &#39;_id&#39;: &#39;$symbol&#39;, &#39;index&#39;: {&#39;$push&#39;: &#39;$date&#39;} } } ] symbols = [(stock[&quot;yahoo&quot;], stock[&quot;insId&quot;]) for stock in client.prod.stock_info.find()] print(&quot;Number of symbols:&quot;, len(symbols)) . Number of symbols: 1780 . Download loop . This is where the magic happens. This for-loop loops through the list of symbols previously retrieved and performs these steps: . Downloads the data. | Removes rows with NaN values. | Detaches the index (dates) and adds it as a column. | Makes all the column names lowercase. | Adds the symbol to every row. This is an important field for when every row becomes a document. | Adds the date and time when the data was retrieved. | Adds a string as the primary key. This a second security measure to prevents the adding of the same date twice to the database. | Removes the already present rows from the data we are going to add to the database. This uses the abovementioned aggregation query. | Finally, it converts the tabular data to documents (dictionaries) and inserts them into the database. | . This loop may take a long time to finish. Especially when the number of stocks is large. Some APIs might stop giving you data by blocking your requests a long time before all the symbols have been looped through. This is one of the reasons by Börsdata is used. Although it is a payed service, it does not limit the number of requests. If this is not an option for you, YahooFinance&#39;s API can be used. The pros are that they have all the stocks you could ever want and are free, but they start blocking requests after a couple of hundred queries. . progress = tqdm(symbols) for symbol, insId in progress: progress.set_description(symbol) # Set progress bar description df = api.get_instrument_stock_prices(insId) # Download the data #df = data[symbol] df = df.dropna() df.reset_index(level=0, inplace=True) df.columns = [column.lower() for column in df.columns] df[&quot;symbol&quot;] = symbol df[&quot;updated&quot;] = datetime.now() df[&quot;_id&quot;] = df[&quot;symbol&quot;] + &quot;:&quot; + df[&quot;date&quot;].astype(str) aggreg_query[0][&quot;$match&quot;][&quot;symbol&quot;] = symbol index = list(daily.aggregate(aggreg_query)) if len(index)&gt;0: # Check if stock already in db index=index[0] index = pd.Series(index[&quot;index&quot;]) df = df[~df[&#39;date&#39;].isin(index)] # Ignoring indicies already in db if len(df)&gt;0: daily.insert_many(df.to_dict(&#39;records&#39;)) . Cronjob . To make the data retrieval automatic it has to be scheduled. This can easily be done using cronjob on linux machines. The following script will be made into a cronjob and this parameter will be added to the crontab 0 18 * * 1-5 data_downloader/daily_downloader.py. The parameter means that the command should be run every weekday at 18 o&#39;clock. . #- #!data_downloader/env/bin/python import yfinance as yf import pymongo from tqdm import tqdm from datetime import datetime import pandas as pd import borsdata_api as api api = api.BorsdataAPI(open(&quot;api.txt&quot;, &quot;r&quot;).read()) # Börsdata is used as data source client = pymongo.MongoClient(&#39;192.168.1.38&#39;, 27017) # MongoDB server connection daily = client.prod.daily # The collection used to store daily data aggreg_query = [ { &#39;$match&#39;: {&#39;symbol&#39;: None} }, { &#39;$group&#39;: { &#39;_id&#39;: &#39;$symbol&#39;, &#39;index&#39;: {&#39;$push&#39;: &#39;$date&#39;} } } ] symbols = [(stock[&quot;yahoo&quot;], stock[&quot;insId&quot;]) for stock in client.prod.stock_info.find()] print(&quot;Number of symbols:&quot;, len(symbols)) progress = tqdm(symbols) for symbol, insId in progress: progress.set_description(symbol) # Set progress bar description df = api.get_instrument_stock_prices(insId) # Download the data #df = data[symbol] df = df.dropna() df.reset_index(level=0, inplace=True) df.columns = [column.lower() for column in df.columns] df[&quot;symbol&quot;] = symbol df[&quot;updated&quot;] = datetime.now() df[&quot;_id&quot;] = df[&quot;symbol&quot;] + &quot;:&quot; + df[&quot;date&quot;].astype(str) aggreg_query[0][&quot;$match&quot;][&quot;symbol&quot;] = symbol index = list(daily.aggregate(aggreg_query)) if len(index)&gt;0: # Check if stock already in db index=index[0] index = pd.Series(index[&quot;index&quot;]) df = df[~df[&#39;date&#39;].isin(index)] # Ignoring indicies already in db if len(df)&gt;0: daily.insert_many(df.to_dict(&#39;records&#39;)) .",
            "url": "https://nicolo.io/finance/data/2021/06/28/Data-Retrieval.html",
            "relUrl": "/finance/data/2021/06/28/Data-Retrieval.html",
            "date": " • Jun 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "The goal of this blog is to write about topics that I find interesting. Categories that will be in focus are computer science and finance, my two favorite domains. I like reading academical papers so I will sprinkle links to these in all relevant places. .",
          "url": "https://nicolo.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nicolo.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}